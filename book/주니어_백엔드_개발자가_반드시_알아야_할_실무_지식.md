### ToC

- [2장. 느려진 서비스, 어디부터 봐야 할까](#2장-느려진-서비스-어디부터-봐야-할까)
  - [응답 시간(Response Time)](#응답-시간response-time)
  - [처리량(Throughput)](#처리량throughput)
  - [병목 지점(Bottleneck) 찾기](#병목-지점bottleneck-찾기)
  - [DB 커넥션 풀](#db-커넥션-풀)
    - [커넥션 풀 크기](#커넥션-풀-크기)
    - [커넥션 대기 시간](#커넥션-대기-시간)
    - [최대 유휴시간, 유효성 검사, 최대 유지 시간](#최대-유휴시간-유효성-검사-최대-유지-시간)
  - [서버 캐시](#서버-캐시)
    - [적중률과 삭제 규칙](#적중률과-삭제-규칙)
    - [로컬(Local) 캐시와 리모트(Remote) 캐시](#로컬local-캐시와-리모트remote-캐시)
    - [캐시 사전 적재](#캐시-사전-적재)
    - [캐시 무효화](#캐시-무효화)
  - [가비지 컬렉터와 메모리 사용](#가비지-컬렉터와-메모리-사용)
  - [응답 데이터 압축](#응답-데이터-압축)
  - [정적 자원과 브라우저 캐시](#정적-자원과-브라우저-캐시)
  - [정적 자원과 CDN](#정적-자원과-cdn)
  - [대기 처리](#대기-처리)

# 2장. 느려진 서비스, 어디부터 봐야 할까

다양한 지표가 성능과 관련이 있다. 네트워크 속도, 디스크 속도, CPU 속도, 메모리 크기, GC 속도 등 다양한 지표가 성능과 관련이 있다.
이런 다양한 지표 중에서 서버 성능과 관련 있는 중요한 지표를 2가지 꼽자면 응답 시간(Response Time)과 처리량(Throughput)이다.

## 응답 시간(Response Time)

```mermaid
sequenceDiagram
    participant App as 앱
    participant APIServer as API 서버
    participant DBMS

    %% 1. 앱이 API 서버에 요청을 보냄
    App->>APIServer: 요청(Request)
    %% 2. API 서버가 DBMS에 데이터 질의
    APIServer->>DBMS: 데이터 질의(Query)
    %% 3. DBMS가 API 서버에 질의 결과 반환
    DBMS-->>APIServer: 질의 결과(Result)
    %% 4. API 서버가 앱에 응답 반환
    APIServer-->>App: 응답(Response)
```

**그림 1.** 앱, API 서버, DBMS 간의 시퀀스 다이어그램

1. API 요청: 서버에 연결 + 서버로 데이터 전송
2. SQL 실행, 응답 생성 등: 서버 실행
3. API 응답: 클라이언트로 데이터 전송

응답 시간은 다음과 같이 2가지로 나누어 측정하기도 한다.

- TTFB(Time to First Byte): 첫 번째 바이트가 도착하기까지의 시간
- TTLB(Time to Last Byte): 마지막 바이트가 도착하기까지의 시간

파일 다운로드처럼 전송할 데이터가 크거나 네트워크 속도가 느리면 TTFB와 TTLB의 차이가 커질 수 있다.

서버 개발자는 주로 서버의 처리 시간을 확인한다. 서버 처리 시간은 다음과 같은 요소를 포함한다.

- 로직 수행(if, for 등)
- DB 연동(SQL 실행)
- 외부 API 연동
- 응답 데이터 생성(전송)

이 중에서도 DB 연동과 외부 API 연동이 큰 비중을 차지한다. 실제 한 요청의 처리 시간을 측정한 결과다.

- 전체 처리 시간: 348ms
- API 연동 1(External API) 186ms(53%)
- API 연동 2(Internal API) 44ms(13%)
- DB 연동(SQL 실행 6회): 101ms(29%)
- 응답 데이터 생성: 17ms(5%)

이처럼 API 연동과 DB 연동이 전체 처리 시간의 대부분을 차지하는 경우가 많다. 이러한 이유로 응답 시간을 줄일 때 DB 연동과 API 연동 시간에 집중한다.

## 처리량(Throughput)

처리량은 단위 시간당 처리할 수 있는 요청의 수를 의미한다. 흔히 TPS(Transaction Per Second: 초당 처리한 트랜잭션 수), RPS(Request Per Second) 등으로 표현한다.

최대 TPS는 시스템이 처리할 수 있는 최대 요청 수를 의미한다. 동시에 들어오는 요청 수가 최대 TPS를 초과하면 서버는 초과한 요청을 나중에 처리한다. 예를 들어, 최대 TPS가 5인 서버에 동시에 7개의 요청이 들어오면 5개는 즉시 처리하고 나머지 2개는 대기한다. 나머지 2개는 5개의 요청이 끝난 후에야 처리된다. 사용자 입장에서는 나중에 처리된 2개의 요청은 **실제 처리된 시간과 더불어서 대기 시간도 포함된다.**

응답 시간의 증가는 사용자 이탈로 이어질 수 있다. 이를 방지하려면 다음 2가지 방법을 고려해야 한다.

1. 서버가 동시에 처리할 수 있는 요청 수를 늘려 대기 시간을 줄인다.
2. 처리 시간 자체를 줄여 대기 시간을 줄인다.

> [!TIP]
> 성능을 개선하려면 먼저 현재 서버의 TPS와 응답 시간을 알아야 한다.
> 막연히 성능이 느리다 말하면서 이것저것 시도하면 안 된다.
> 트래픽이 많은 시간대의 TPS와 응답 시간이 얼마인지 측정하고,
> 이 결과를 바탕으로 목표 TPS와 응답 시간을 설정하고 효과적인 성능 개선안을 도출해야한다.
>
> **TPS를 확인하는 가장 간단한 방법은 모니터링 시스템을 활용하는 것이다.**

## 병목 지점(Bottleneck) 찾기

트래픽이 증가하면서 성능 문제가 발생하는 주된 이유는 시스템이 수용할 수 있는 최대 TPS를 초과하는 트래픽이 유입되기 때문이다. 시스템이 제공할 수 있는 최대 TPS를 높이지 않으면 증가하는 트래픽을 적절히 처리할 수 없다.

TPS를 높이려면 먼저 성능 문제가 발생하는 지점을 찾아야 한다. 문제 지점을 찾는 간단한 방법은 처리 시간이 오래 걸리는 작업을 식별하는 것이다.

TPS를 높이기 위해 무턱대고 서버를 추가해서는 안 된다.

> [!TIP]
>
> **병목 지점(Bottleneck)을 확인하는 가장 간단한 방법은 APM 시스템을 활용하는 것이다.**

## DB 커넥션 풀

DB를 사용하려면 다음과 같이 3단계의 과정이 필요하다.

1. DB에 연결한다.
2. SQL을 실행한다.
3. 사용이 끝나면 DB 연결을 종료한다.

서버와 DB는 네트워크 통신을 통해 연결된다. 이때 네트워크 연결을 생성하고 종료하는 데 걸리는 시간은 0.5초에서 1초 이상 소요되기도 한다. 이 시간이 길게 느껴지지 않을 수도 있지만 실제로는 매우 긴 시간이다. 예를 들어, 10ms에 불과한 짧은 쿼리를 실행하기 위해 연결과 종료에 50ms(0.05초)가 소요된다면 전체 처리 시간은 60ms(0.06초)가 된다. 단순히 계산해도 전체 처리 시간의 80% 이상이 DB 연결 및 종료에 쓰이게 된다.

매 요청마다 DB를 연결하고 종료하면 트래픽이 증가하면 이러한 현상은 더 두드러진다. 매 요청마다 DB를 연결하고 종료하면 트래픽이 증가할 때 급격하게 처리량이 떨어지기도 한다. 이런 문제를 피하기 위해 DB 커넥션 풀을 사용한다. DB 커넥션 풀은 DB에 연결된 커넥션을 미리 생성해서 보관한다. 애플리케이션은 DB 작업이 필요할 때 풀에서 커넥션을 가져와 사용하고, 작업이 끝나면 다시 풀에 반환한다.

커넥션 풀 설정중 중요한 설정은 다음과 같다.

- 커넥션 풀 크기(또는 최소, 최대 크기)
- 풀에 커넥션이 없을 때 커넥션을 구할 때까지 대기할 시간
- 커넥션의 유지 시간(최대 유휴 시간, 최대 유지 시간)

### 커넥션 풀 크기

커넥션 풀 크기는 커넥션 풀에 보관할 수 있는 커넥션의 개수를 의미한다. 서버는 주로 DB와 통신을 하기 때문에, DB 커넥션 풀 크기를 잘못 설정하면 성능에 큰 영향을 미친다. 다음과 같은 상황을 가정해보자.

- 커넥션 풀 크기는 5다.
- 한 요청에서 쿼리를 실행하는데 1초 걸린다.
- 계산을 쉽게 하기 위해 데이터 전송 시간은 무시한다.

서버에 6개 요청이 동시에 들어왔을 때 이 중 5개 요청은 풀에서 커넥션을 가져올 수 있다. 반면 1개 요청은 다른 요청이 커넥션을 반환할 때까지 대기해야 한다.

커넥션 풀의 구현 방식에 따라 다르지만 일반적인 커넥션 풀은 최소 크기와 최대 크기를 설정할 수 있다. 최소 크기는 커넥션 풀에 항상 유지할 커넥션의 개수를 의미한다. 커넥션 풀의 크기를 5로 설정하고 최소 크기를 2로 설정하면 커넥션 풀은 항상 2개의 커넥션을 유지한다. 나머지 3개는 필요할 때마다 생성된다. 이때 커넥션 풀의 크기를 5로 설정하면 최대 5개의 커넥션을 유지할 수 있다.

> [!Tip]
> 트래픽이 순간적으로 급증하는 패턴을 보인다면 커넥션 풀의 최소 크기를 최대 크기에 맞추는 것이 좋다.
> 트래픽이 점진적으로 증가할 때는 DB 연결 시간이 성능에 큰 영향을 주지 않지만 트래픽이 급증할 때는
> DB 연결도 성능 저하의 주요 원인이 될 수 있기 때문이다.

### 커넥션 대기 시간

대기 시간이란 풀에 사용할 수 있는 커넥션이 없을 때 커넥션을 얻기 위해 기다릴 수 있는 최대 시간을 의미한다. 지정된 대기 시간 안에 커넥션을 얻지 못하면 DB 연결 실패 에러가 발생한다.

> [!Tip]
> 따라서 응답 시간이 중요한 서비스는 커넥션 대기 시간을 가능한 한 짧게 설정해야 한다.
> 트래픽의 양이나 서비스의 특성에 따라 차이는 있지만 보통의 경우라면 0.5초에서 3초 이내로 지정하자.
> **모니터링 시스템(APM 등)을 통해 커넥션 대기 시간, 실패율, 응답 시간 등의 지표를 수집한다.**  
> **실시간성이 중요한 서비스라면 짧게, 배치성이나 대기 허용 서비스라면 길게 설정한다.**  
> **금융 거래처럼 반드시 처리가 되어야 하는 경우라면 대기 시간을 더 길게 설정한다.**
>
> **실제 적용 방법**
>
> 1. 초기에는 0.5초~3초로 설정한다. (예: 500ms ~ 3000ms)
> 2. 모니터링
>
>    - 커넥션 풀에서 대기 시간이 자주 발생하는지
>    - 대기 시간 초과로 실패하는 요청이 얼마나 되는지 모니터링
>
> 3. 조정
>
>    - 대기 시간 초과가 많으면 풀 크기를 늘리거나, 대기 시간을 늘리거나, 쿼리 성능을 개선하는 튜닝이 필요하다.

대기 시간을 짧게 설정하면 커넥션 풀이 모두 사용중일 때 빠르게 '일시적 오류'와 같은 에러 응답을 사용자에게 보여줄 수 있다. 에러를 응답하는게 부정적으로 보일 수도 있다.
하지만 대기 시간 때문에 긴 시간 동안 무응답 상태가 되는 것보다는 나을 수 있다. 커넥션을 얻지 못했을 때 에러를 응답해야 서버의 부하가 증가하는 것도 방지할 수 있다.

### 최대 유휴시간, 유효성 검사, 최대 유지 시간

근무 시간대에는 지속적으로 서버에 요청이 들어오지만 새벽 시간대에는 요청이 거의 없을 것이다. 요청이 없는 시간대에는 풀에 있는 커넥션도 사용되지 않는다.
이때 주의할 점이 있다. 커넥션이 사용되지 않는 시간이 길어지면 연결이 끊길 수 있다.

DB와의 연결이 끊긴 커넥션을 사용하면 에러가 발생한다. 이러한 연결 끊김으로 인해 발생하는 에러를 방지하기 위해 커넥션 풀은 다음 2가지 기능을 제공한다.

- 최대 유휴시간(Maximum Idle Time) 지정
- 유효성 검사(Validation) 지원

최대 유휴 시간은 사용되지 않는 커넥션을 풀에 유지할 수 있는 최대 시간을 의미한다.
최대 유휴 시간을 30분으로 설정하면 30분 이상 사용되지 않은 커넥션은 종료되어 풀에서 제거된다.
이 시간을 DB에 설정된 비활성화 유지 시간보다 짧게 설정하면, DB가 연결을 끊기 전에 풀에서 커넥션을 제거할 수 있다.

유효성 검사는 커넥션이 정상적으로 사용할 수 있는 상태인지 여부를 확인하는 절차이다.
이 과정을 통해 커넥션 풀에 존재하는 커넥션 중 연결이 유요하지 않은 커넥션을 식별하고 풀에서 제거할 수 있다.
일부 커넥션 풀은 유효성 검사를 위해 실제 쿼리를 실행하기도 한다. 이때는 `SELECT 1`, `SELECT 1 FROM DUAL`과 같은 간단한 쿼리를 사용한다.

커넥션 풀이 제공하는 또 다른 설정은 최대 유지 시간이다. 이 갑싱 4시간으로 설정되면 커넥션은 생성된 후 최대 4시간 동안만 유지된다. 4시간이 지나면 커넥션은 유효하더라도 커넥션을 닫고 풀에서 제거된다.

> [!TIP]
> 최대 유휴 시간과 최대 유지 시간을 무한대로 설정하지 않는 것이 좋다.
> 커넥션 풀의 기본값을 확인한 뒤 이 두 설정의 기본값이 무제한으로 되어 있다면 DB 설정을 참고하여 알맞게 적절한 값으로 지정해야 한다.

## 서버 캐시

DB 서버를 수평 확장하더라도 처리량을 늘릴 수 있지만 실행 시간이 획기적으로 줄어들지는 않는다.
DB 서버를 확장하지 않고도 응답 시간과 처리량을 개선하고 싶다면 캐시(Cache) 사용을 고려할 수 있다.

```mermaid
---
title: 캐시의 동작 방식
---

stateDiagram-v2
    getFromCache: 캐시에서 해당하는 값 조회
    [*] --> getFromCache

    state isEmpty <<choice>>
    getFromCache --> isEmpty

    useCache: 값 사용
    isEmpty --> useCache: [값 존재]

    getFromDB: DB에서 값 조회
    isEmpty --> getFromDB: [값 없음]
    saveCache: (키, 값)을 캐시에 저장
    getFromDB --> saveCache
    saveCache --> useCache
    useCache --> [*]
```

> [!NOTE]
> 적절한 캐시 키를 선택해야한다. 예를 들어
> 게시글 상세 정보는 "artices:번호" 형태의 캐시 키를 사용하고 최신 인기 글은 "articles:hot10" 형태의 캐시 키를 사용하는 식이다.
> 또한 캐시를 사용할 때는 캐시 키가 겹치지 않도록 주의해야 한다.

### 적중률과 삭제 규칙

캐시에 보관할 수 있는 데이터에 제한이 있으므로, 캐시가 가득 차 있는 상태에서 데이터를 캐시에 저장하면 기존에 있는 데이터 중 하나를 제거해야 한다.
삭제할 대상을 선택할 때 주로 사용하는 규칙은 다음과 같다.

- LRU(Least Recently Used): 가장 오래전에 사용된 데이터를 삭제한다.
- LFU(Least Frequently Used): 가장 적게 사용된 데이터를 삭제한다.
- FIFO(First In First Out): 먼저 추가된 데이터를 삭제한다.

오래된 데이터는 캐시에 있어도 사용되지 않을 가능성이 크다.
따라서 캐시가 가득 차 있지 않더라도 오래된 데이터는 미리 삭제하는 것이 좋다.
이를 위해 캐시에는 유효 시간(TTL, i.g.만료 시간)을 설정하는 방식도 함께 사용한다.

### 로컬(Local) 캐시와 리모트(Remote) 캐시

서버가 사용하는 캐시는 크게 두 종류가 있다. 첫 번째는 로컬(Local) 캐시다.
로컬 캐시는 서버 프로세스와 동일한 메모리를 캐시 저장소로 사용한다.
두 번째는 리모트(Remote) 캐시다. 리모트 캐시는 별도 프로세스를 캐시 저장소로 사용한다.

로컬 캐시 구현 기술로는 Caffeine(Java), go-cache(Go), node-cache(Node.js)등이 있다.
로컬 캐시의 장점은 속도에 있다.
또한 별도의 외부 연동이 필요하지 않아 구조를 단순하게 유지할 수 있다.

| 구분          | 로컬(Local) 캐시                                                                                                                                           | 리모트(Remote) 캐시                                                                                                                                                |
| :------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **장점**      | - 매우 빠른 응답 속도 (메모리 접근)<br>• 네트워크 지연 없음<br> - 구현이 단순하고 외부 의존성 없음<br> - 외부 시스템 연동 불필요                           | - 여러 서버 간 캐시 데이터 공유 가능<br> -서버 확장에도 일관된 캐시 유지<br> - 서버 재시작해도 캐시 유지<br> - 대용량 데이터 캐싱 가능<br> - 중앙 집중식 캐시 관리 |
| **단점**      | - 서버마다 별도 캐시 관리 필요<br> - 서버 재시작 시 캐시 소실<br> - 서버 간 캐시 데이터 불일치 발생<br> - 메모리 제약으로 캐시 크기 제한<br> - 확장성 제한 | - 네트워크 지연 발생<br> - 외부 시스템 의존성 증가<br> - 구현 및 운영 복잡도 증가<br> - 네트워크 비용 발생<br> - 외부 서비스 장애 시 영향                          |
| **적합 사례** | - 변경이 적은 정적 데이터<br> - 서버별 독립적 데이터<br> - 초고속 응답이 필요한 경우<br> - 단일 서버 환경                                                  | - 다중 서버 환경<br> - 일관된 캐시 데이터 필요 시<br> - 대규모 캐시 데이터 필요 시<br> - 서버 재시작에도 데이터 유지 필요 시                                       |
| **구현 기술** | - Caffeine (Java)<br> - go-cache (Go)<br> - node-cache (Node.js)                                                                                           | - Valkey<br> - Memcached<br> - Hazelcast                                                                                                                           |

로컬 캐시와 리모트 캐시는 각각 장단점이 뚜렷하기 때문에 상황이나 용도에 맞게 선택해야 한다.
어떤 방식을 무조건 선택해야 한다는 절대적인 기준은 없으며 데이터 규모, 변경 빈도, 응답 시간, 처리량 등을 판단 기준으로 결정해야 한다.

캐시에 보관할 데이터 규모가 작고 변경 빈도가 매우 낮다면 로컬 캐시로 충분하다.
예를 들어 홈 화면에 표시할 최신 공지글 목록을 생각해보자. 보통 이 목록은 많아야 10개 미만이다.
또한 몇 시간에서 며칠 동안 동일할 때가 많다.
이처럼 자주 바뀌지 않고 크기가 작은 데이터는 로컬 캐시를 사용하기에 적당하다.

반면에 데이터 규모가 크다면 리모트 캐시를 사용해야 한다.
대형 이커머스 사이트의 개별 제품 정보가 그 예다.
로컬 캐시로는 메모리 용량 등의 한계로 대응이 어렵다.
일부 데이터를 로컬 캐시에 저장하더라도 데이터가 수시로 변경되면 캐시 효율도 떨어진다.

### 캐시 사전 적재

트래픽이 순간적으로 급증하는 패턴을 보인다면 캐시에 데이터를 미리 저장하는 것도 고려할 필요가 있다.
다음은 캐시에 미리 데이터를 저장하면 큰 효과를 볼 수 있는 가상의 사례다.

- G 앱 사용자는 300만명
- 매달 정해진 날에 이달의 요금 정보를 보여준다.
- 해당 일자가 되면 전체 회원을 대상으로 요금 안내 푸시 알림을 발송한다.
- 푸시를 받은 사용자 중 일부는 이달의 요금 정보를 조회한다.

대응 방법은 캐시에 데이터를 미리 넣어두는 것이다.
각 사용자의 요금 정보를 캐시에 저장해두면 푸시를 받은 사용자가 한꺼번에 몰려올 때도 캐시 적중률을 99%에 가깝게 유지할 수 있다.
이를 통해 순간적으로 트래픽이 몰렸을 때도 응답 시간을 안정적으로 유지할 수 있으며, DB에 부하가 집중되는 현상도 효과적으로 방지할 수 있다.

### 캐시 무효화

캐시에 보고나된 데이터의 원본이 바뀌면, 그에 맞춰 캐시에 보관된 데이터도 함께 변경하거나 삭제해야 한다.
원본이 변경됐는데 캐시에 저장된 데이터가 갱신되지 않으면 사용자는 오래된 잘못된 정보를 확인하게 되는 문제가 발생할 수 있다.

캐시에 저장된 데이터의 특성에 따라 캐시를 무효화하는 시점을 달리 설정해야 한다.
가격 정보, 게시글 내용처럼 민감한 데이터는 변경되는 즉시 캐시를 무효화해야 한다.
게시글 내용을 수정했는데도 캐시가 그대로 유지되면 사용자는 수정 전 게시글 내용을 보게 되어 혼란을 겪을 수 있다.
작성자는 수정이 제대로 반영되지 않았다고 생각해 오류 신고를 하거나 서비스에 대한 신뢰도를 낮게 평가할 수 있다.

변경에 민감한 데이터는 로컬 캐시가 아닌 리모트 캐시에 보관해야 한다.
A 서버에 연결한 사용자는 변경된 가격 정보를 보지만 B 서버에 연결한 사용자는 B 서버의 로컬 캐시에 보관된 변경전 가격 정보를 보게 되는 문제가 발생할 수 있어 서비스 신뢰성에 큰 영향을 줄 수 있다.

변경에 민감하지 않고 데이터 크기가 작다면 캐시의 유효 시간을 설정하여 주기적으로 갱신하는 방식을 사용해도 된다.
예를 들어, 최근 인기 글 목록을 캐시에 저장한 경우, 최근 인기 글 목록이 바뀌고 몇 분 뒤에 캐시 데이터가 변경되더라도 서비스에 심각한 문제는 일어나지 않는다.
인기 글 목록을 저장하는 캐시의 유효 시간을 10분으로 지정하면 10분 주기로 최신 인기 게시글 목록을 갱신하는 효과를 얻을 수 있다. 갱신 시간을 줄이고 싶으면 유효 시간을 10분에서 5분으로 줄이기만 하면 된다.

## 가비지 컬렉터와 메모리 사용

가비지 컬렉터(Garbage Collector, GC)를 사용하는 언어는 사용이 끝난 객체를 힙 메모리에서 바로 삭제하지 않고 정해진 규칙에 따라 사용하지 않는 메모리를 정리한다.

메모리를 많이 사용하고 생성된 객체가 많을수록 사용하지 않는 객체를 찾는 데 시간이 오래 걸린다.
GC 알고리즘과 메모리 사용 패턴에 따라 차이가 있지만 사용하는 메모리양과 객체 수가 많을 수록 GC가 실행되는 시간이 길어진다.

반대로 메모리 사용을 줄이면 GC가 실행되는 시간도 줄어든다.
물론 실제 애플리케이션이 4GiB에 가까운 메모리가 있어야 하는데도 2GiB로 설정하면 OOM(Out Of Memory) 에러가 발생할 수 있으므로 실제 메모리 사용 패턴에 맞게 최대 힙 크기를 조정해야 한다.

한 번에 대량으로 객체르 생성하는 것도 주의해야 한다.
게시글 하나가 0.5KiB라고 가정했을 때 10만 개의 게시글은 대략 50MiB의 메모리를 사용한다.
만약 동시에 100명 사용자가 게시글을 조회한다면 5GiB의 메모리를 사용하게 된다.
사용할 수 있는 최대 메모리가 4GiB라면 OOM 에러가 발생할 수 있다.

대량으로 객체가 생성되는 것을 방지하려면 조회 범위를 제한해야 한다.
10년 치 거래 내역을 한버에 조회할 수 있도록 하기보다는 최대 3개월 치만 조회할 수 있도록 한다.
마찬가지로 한 번에 조회할 수 있는 데이터의 개수도 트래픽 규모와 메모리 크기에 맞춰 제한해야 한다.

파일 다운로드와 같은 기능을 구현할 때는 스트림을 활용한다.
다음 자바 코드 처럼 파일 데이터를 한꺼번에 메모리에 로딩한 후에 응답하는 방식은 피해야 한다.
파일 크기와 동시 사용자 수에 따라 메모리 사용량이 급증할 수 있기 때문이다.
이 코드는 30MiB 크기의 파일을 100명이 동시에 다운로드하면 3GiB의 메모리를 사용하게 된다.

```java
var bytes = Files.readAllBytes(Paths.get("file.txt")); // 파일을 한 번에 메모리에 로딩
out.write(bytes);
```

스트림을 활용하면 파일 처리 과정에서 필요한 메모리 크기를 줄일 수 있다. 다음은 스트림을 이용하도록 변환한 코드이다.

```java
InputStream in = Files.newInputStream(Paths.get("file.txt")); // 파일을 스트림으로 읽기
byte[] buffer = new byte[8192]; // 8KiB 버퍼

int read;
while ((read = in.read(buffer)) != -1) {
    out.write(buffer, 0, read); // 버퍼에 있는 데이터만 전송
}
```

이 코드는 파일을 한 번에 읽지 않고 8KiB씩 끊어 읽는다. 동시에 100명이 다운로드를 요청하더라도 메모리 사용량은 800KiB에 불과하다. 파일 전체를 한 번에 메모리에 로딩하는 방식은 3GiB의 메모리를 사용하지만 스트림을 이용하면 800KiB의 메모리만 사용한다. 이처럼 스트림을 활용하면 메모리 사용량을 줄일 수 있다.

> [!NOTE]
> 문제 원인을 분석하기 위해 힘 덤프를 저장하고 서버를 재시작했다.
>
> 엑셀 다운로드 기능을 실행 → 조회한 데이터가 수백만 건 → 모든 데이터가 메모리에 로딩됨  
> → 엑셀 생성 시간이 길어지자 사용자는 요청을 취소하고 다시 요청함  
> → OOM 발생

> **해결:**
>
> - 엑셀을 메모리에서 생성하지 않고 로컬 파일에 스트림 형태로 만들도록 변경
> - DB에서 조회한 결과를 리스트로 한 번에 받아 처리하던 것을 스트림 형태로 받아 순차적으로 처리하는 방식으로 변경

## 응답 데이터 압축

응답 시간에는 데이터 전송 시간이 포함된다. 이 전송 시간은 2가지 요인에 영향을 받는다.

- 네트워크 속도
- 전송 데이터 크기

사용자의 네트워크 속도가 느리면 응답 시간이 길어진다.
하나의 무선 공유기에 너무 많은 사용자가 붙었을 때 웹사이트가 느려지는 것과 같은 원리다.
전송할 데이터의 크기가 커도 응답 시간이 길어진다.
10KiB 크기의 파일을 다운로드받는 시간 보다 1GiB 크기의 파일을 다운로드받는 시간이 더 긴 것과 같다.

서버는 사용자즈이 네트워크 속도를 제어할 수 없지만 전송하는 데이터의 크기는 제어할 수 있다.
이때 사용할 수 있는 방법은 응답 데이터를 압축해서 전송하는 것이다.
실제로 gzip으로 압축하면 데이터 크기를 50% 이상 줄일 수 있다.
전송 크기가 줄어든 만큼 응답 시간도 줄어든다.
데이터 전송량을 줄이는 것은 응답 시간뿐만 아니라 비용에도 영향을 준다.
Nginx와 같은 웹 서버는 압축 기능을 제공하고 있으므로 약간의 설정만 추가하면 즉시 효과를 볼 수 있다.

> [!NOTE]
> 웹 브라우저나 HTTP 클라이언트는 Accept-Encoding 헤더를 통해 서버에 처리할 수 있는 압축 알고리즘을 알린다.
> 예를 들어, gzip, deflate 알고리즘을 사용해서 압축을 풀 수 있다면 다음과 같이 헤더를 설정한다.]
>
> ```text
> Accept-Encoding: gzip, deflate
> ```
>
> 웹 서버는 Accept-Encoding 헤더를 확인한 후 지원하는 압축 알고리즘으로 응답 데이터를 압축한다.
> 이때 사용된 압축 알고리즘은 Content-Encoding 헤더에 포함되어 응답된다.
>
> **📜 핵심 정리**
>
> - 압축은 가장 바깥 레이어에서 1회만 처리하는 것이 좋다.
> - 이중 압축 = 데이터 전송 효율성 ↓ + CPU 낭비 ↑ + 오류 ↑
> - Best case: Reverse Proxy 혹은 Load Balancer에서 압축 + 애플리케이션은 순수 데이터 전송

응답 데이터를 압축할 때는 다음과 같은 점을 고려해야 한다.

- html, css, js와 같은 텍스트 파일은 압축률이 높다. 반면 jpeg, zip 파일 처럼 이미 압축된 파일은 압축률이 낮다. 따라서 모든 파일에 압축을 적용하는 것보다는 텍스트 파일에만 압축을 적용하는 것이 좋다.
- 웹 서버(HAProxy, Nginx, etc.)에 압축을 적용했더라도 방화벽이 이를 해제해 응답할 수 있다.

## 정적 자원과 브라우저 캐시

서버는 2가지 종류의 데이터를 응답한다. 하나는 동적 데이터, 다른 하나는 정적 데이터다.
동적 자원은 브라우저가 요청할 때마다 결과가 바뀌는 데이터로 제품 목록 HTML 이나 제품 상세 JSON과 같은 데이터다.
정적 자원은 요청할 때마다 결과가 바뀌지 않는 데이터로 이미지, CSS, JS 파일과 같은 데이터다.
정적 자원은 전체 트래픽에서 상당한 비중을 차지한다. 이미지가 많은 온라인 쇼핑몰 사이트의 경우 전체 트래픽의 80%를 차지하기도 한다.

서버가 전송하는 트래픽을 줄이면서 브라우저가 정적 자원을 캐시할 수 있도록 설정하는 것이 좋다.
HTTP 프로토콜에서는 데이터를 응답할 때 Cache-Control 헤더를 통해 브라우저 캐시 설정을 지원한다.

```text
Cache-Control: max-age=60
```

## 정적 자원과 CDN

브라우저 캐시를 사용하면 네트워크 트래픽을 줄일 수 있지만, 브라우저 캐시는 브라우저 단위로 동작하기 때문에 동시에 많은 사용자가 요청할 경우 순간적으로 많은 양의 이미지, JS, CSS 파일을 전송하게 된다. 이로 인해 빠르게 네트워크가 포화되어 응답 시간이 급격하게 느려진다. 4차선 도로(대역폭)에 차가 1~2대씩 지나가면 막힘없이 잘 통과하지만 수십 대의 차가 몰리면 도로가 막히는 것과 같은 원리다.

이런 문제를 해결하는 방법 중 하나가 CDN(Content Delivery Network)이다.
이름에서 알 수 있듯이 CDN은 콘텐츠를 제공하기 위한 별도의 네트워크를 의미한다.
사용자는 CDN이 제공하는 서버에 요청을 보내고 CDN은 사용자의 요청을 처리하기 위해 가장 가까운 CDN 엣지 서버(Edge Server)에 요청을 전달한다.
CDN 서버에 요청한 콘텐츠가 없으면 오리진 서버(Origin Server)에서 콘텐츠를 가져온다.
오리진 서버에 읽어온 콘텐츠는 CDN 서버에 캐시된다.

## 대기 처리

사용자가 순간 폭증할 때가 있다. 대표적인 에가 콘서트 예매다.
이렇게 짧은 시간 동안 폭증하는 트래픽은 어떻게 처리해야 할까? 서버 증설, DB 증설 ...
짧은 시간을 버티기 위해서 투입해야 하는 비용이 크다.
또한 클라우드에서 증설한 서버는 다시 줄일 수 있지만 DB는 그렇지 않다. 최대 트래픽에 맞춰 DB 성능을 Scale Up하면 다시 DB 성능을 줄이기가 어렵다.
전체 서비스 시간 1%도 되지 않는 시간을 위해 고정 비용(DB 비용)이 커지는 격이다.

이런 문제를 해결하기 위해 대기 처리 시스템을 도입할 수 있다.
처리할 수 있는 시스템의 처리량을 무작정 늘리기보다는 수용할 수 있는 수준의 트래픽만 받아들이고 나머지는 대기 처리 하는 것이다.

> [!NOTE]
> 서버 개발자는 대규모 트래픽 대응을 위한 아키텍처 + 비용 절감 방안을 고민해야 한다.
